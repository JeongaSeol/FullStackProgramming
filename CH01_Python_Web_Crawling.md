# Python Web Crawling



## 크롤링(Crawling)과 스크래핑(Scraping)



- ### 크롤러(Crawler) vs 크롤링(Crawling)

  - __Crawler(크롤러)__ : 웹 페이지를 방문해 정보를 자동적으로 수집해오는 프로그램
    - 대규모의 정보를 단시간에 수집
  - __Crawling(크롤링)__ : 크롤러(Crawler)로 정보를 수집하는 작업



- ### 스크래핑(Scraping)

  - 수집한 정보를 분석해서 __필요한 정보를 추출하는 것__
  - __수집 -> 분석 -> 추출 -> 가공 -> 저장 -> 출력__ 순으로 웹 페이지의 정보를 처리



- ### 주의사항

  - __웹 사이트에 접근할 때의 주의 사항__
    - 웹 사이트의 이용 규약을 확인하고 철저히 지킨다.
    - robots.txt와 robots 메타 태그의 접근 제한 사항을 지킨다.
    - 제한이 없더라도 상대 서버에 부하가 가지 않을 정도의 속도로 접근한다.
    - `rel = 'nofollow'`가 설정되어 있으면 크롤러로 접근하지 않는다.
    - 크롤링을 거부하는 조치가 있으면 즉시 크롤링을 멈추고, 이미 추출한 정보를 모두 삭제한다.
  - __수집한 데이터를 다룰 때의 주의 사항__
    - 수집한 데이터는 저작권을 지켜서 사용해야 한다.
    - 저작권에 문제가 있으면 개인적인 용도로만 사용한다.
    - 수집한 데이터를 기반으로 검색 서비스를 제공하는 경우, 
      웹 사이트와 API 등의 사용 규약을 확인하고 문제가 없을 경우에만 제공한다.
    - 이용 규약이 따로 없을 때도 상대방에게 확인한 뒤에 데이터를 공개한다.



## 크롤러(Crawler) 설계 기본



- ### 목적과 대상을 명확하게 하기

  - 개발전에 목적을 명확하게 함
    - 대상 사이트나 데이터를 명확하게 구분 및 선정해야함 
  - 대상을 충분하게 분석하는 것

  

- ### URL 확인하기

  - 사이트맵을 트리구조(페이지)로 제공하는 사이트
    - 사이트맵을 보면 어떤 정보가 어떤 URL 아래에 있는지 확인 가능
    - https://www.seoul.go.kr/helper/siteMap.do

  - 사이트맵을 XML로 제공하는 사이트
    - https://www.usa.gov/sitemap.xml
  - 사이트맵을 확인 할 수 없을 때
    - 카테고리 목록 페이지로 이도하는 링크가 없는지 사이트 내부에서 하나하나 찾아봄

  

- ### 목적 데이터를 따로 제공하는지 확인하기

  - 사이트에 따라서는 불특정 다수의 크롤러가 접근해서 부하가 발생시키는 것을 방지하기 위해
    __공식 아카이브 데이터__를 제공하기도 함
    - EX) https://ko.wikipedia.org/wiki/
  - 아카이브 데이터 덤프를 제공해주지 않는 경우라도 웹 API와 피드를 제공해주면 이를 활용

  

- ### 웹 API

  - 특정 URL에 정해진 매개 변수를 넣어 접근하면 XML 또는 JSON 등의 구조화된 데이터를 제공
    - ex) 네이버 오픈 API
      - https://developers.naver.com/docs/common/openapiguide/apilist.md
    - ex) 카카오 
      - https://developers.kakao.com/

  

- ### 설계가 필요한 부분

  - 목적 : 출력 결과로 무엇이 필요한가
    - 스프레드시트의 특정 위치에 숫자를 반영
    - 다른 시스템과 연동할 수 있게 API를 제공
    - 자신의 사이트에서 읽어 들일 수 있게 피드 생성
  - 출력결과와 관련된 명확한 상세가 있어야 함

  - 크롤러의 각 처리 공정

    <img src="C:\Users\sja95\OneDrive\바탕 화면\크롤러 처리 공정.PNG" alt="크롤러 처리 공정" style="zoom: 80%;" />

  

- ### 네트워크 요청

  - __간격 설정__
    - 적어도 1초에 1번 정도만 요청 할 수 있게 하는 것을 권장
  - __타임 아웃__
    - 요청한 사이트로부터 응답이 오지 않는 경우에 타임아웃 설정
    - 3초 동안 응답이 없으면 멈춤
  - __재시도__
    - 큰 문제가 없는데도 오류를 응답하는 경우
    - 재시도 할 때는 어느 정도의 횟수 제한(약 1~3회 정도)이 있어야 함
    - 재시도 간격도 고려해야 함

  

- ### 파싱(분석)

  - __문자 코드__

    - 대부분 UTP-8로 작성되어 있지만 HTML 소스 코드는 다양한 문자코드로 작성된 경우가 많음
      (EUC-KR 등)

  - __HTML/XML 파싱__

    - 웹 페이지 중에는 태그가 잘못 구성되어 있거나 
      속성 값에 큰 따옴표`" "` 가 생략되어 있는 경우도 많음

  - __JSON 디코드__

    - 대부분의 웹 API는 JSON 형식으로 데이터를 응답

    

- ### 스크래핑과 정규 표현식

  - __URL 정규화__
    - 링크를 추출할 때 링크가 상대 경로인 경우
  - __TEST__
    - 스크래핑 라이브러리를 사용하거나 정규 표현식을 사용하더라도 
      한 번에 원하는 데이터를 추출하기는 어려움
    - 테스트 코드를 사용하면 수집 처리와 스크래핑 처리를 분리하기 쉬움



- ### 데이터 저장소의 구조와 선택
  - ##### 데이터 저장소 

    - 파일
    - 문서 DB
    - 관계형 DB
    - 객체 DB
    - 키-값 DB

  

- ### 배치 생성시 주의점

  - 공정 분리하기
  - 중간 데이터 저장하기
  - 실행 시간 알아 두기
  - 중지 조건 명확히 하기
  - 함수의 매개 변수를 간단히 하기
  - 날짜를 다룰 때 주의하기 



- ### 설계

  - __소스 확인하기__

  - ##### 저장 방법

  - ##### 파일 저장 형식

    - CSV(Comma-Separated Values)
    - TSV(Tab-Separated Values)
    - JSON(JavaScript Object Notation)

